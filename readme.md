# Crazy Bandits

Crazy Bandits is an implementation of a multi-armed bandit setting along
with various bandit algorithms like Thompson Sampling, UCB, Eplison-Greedy and Explore then Commit 
(more to come?). It also includes an analysis of variance and average of cumulative regret in simulations of these
algorithms.
